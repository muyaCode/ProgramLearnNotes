import{_ as s,c as a,o as e,a3 as n}from"./chunks/framework.zGi9i9Bf.js";const r="/ProgramLearnNotes/assets/640.De1ptJ5v.webp",p="/ProgramLearnNotes/assets/640-1713977545935-1.C0tePY5h.webp",t="/ProgramLearnNotes/assets/640-1713977545936-2.DxANSGo6.webp",o="/ProgramLearnNotes/assets/640-1713977545936-3.CHQ9s3t7.webp",l="/ProgramLearnNotes/assets/640-1713977545936-4.BRUyTuoG.webp",i="/ProgramLearnNotes/assets/640-1713977545936-5.CZp4Oxwm.webp",v=JSON.parse('{"title":"Venom-Crawler转为捡洞而生的爬虫神器","description":"","frontmatter":{},"headers":[],"relativePath":"Document/网络安全/【网络安全工具和使用】/Venom-Crawler转为捡洞而生的爬虫神器.md","filePath":"Document/网络安全/【网络安全工具和使用】/Venom-Crawler转为捡洞而生的爬虫神器.md","lastUpdated":1717759497000}'),c={name:"Document/网络安全/【网络安全工具和使用】/Venom-Crawler转为捡洞而生的爬虫神器.md"},m=n(`<h1 id="venom-crawler转为捡洞而生的爬虫神器" tabindex="-1">Venom-Crawler转为捡洞而生的爬虫神器 <a class="header-anchor" href="#venom-crawler转为捡洞而生的爬虫神器" aria-label="Permalink to &quot;Venom-Crawler转为捡洞而生的爬虫神器&quot;">​</a></h1><p><strong>工具介绍</strong></p><p>毒液爬行器Venom-Crawle：为Venom-Transponder专为捡洞而生的一个爬虫神器。</p><p><strong>安装依赖：</strong></p><div class="language- line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki one-dark-pro vp-code" tabindex="0"><code><span class="line"><span>go mod tidy # go mod依赖加载cd cmdgo build . #然后把cmd.exe重命名一下就好</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>使用说明</strong></p><p><strong>不再加入Gospider原因：</strong></p><p>感觉Katana+Crawlergo的爬行结果以及足够完整，再加入Gospider可能会造成时间的大量决策，个人比较倾向于基于Chromium的爬行结果，参数可靠。</p><p><strong>功能介绍：</strong></p><ul><li>为了使爬虫爬行的URL问题完整，所以使用Katana+Crawlergo的方法结合获取所有符合的URL，思路是：先由katana爬行，将爬行的结果最终挖掘Crawlergo再进行二次爬取，左侧脚踩右脚旋转升天。</li><li>如果配置-proxy将流量代理给海绵环境监听的端口（比如：Venom-Transponder、Xray、w13scan等）</li><li>这里防止为了爬偏，爬行规则就是输入的URL路径，不会爬行其他域名以及子域名</li><li>Katana和Crawlergo的结果都会单独保存在txt中，并且result-all.txt是去重后的最终结果</li></ul><div class="language- line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki one-dark-pro vp-code" tabindex="0"><code><span class="line"><span> -headless   是否让爬行时候headless结果可见 </span></span>
<span class="line"><span> -chromium   如果在代码执行过程中报查询不到环境中的浏览器， 将Chrome或者Chromium路径填入即可 </span></span>
<span class="line"><span> -headers    爬行要求带入的JSON字符串格式的自定义请求头，默认只有UA </span></span>
<span class="line"><span> -maxCrawler URL启动的任务最大的爬行个数,这个针对Crawlergo配置 </span></span>
<span class="line"><span> -mode       爬行模式，simple/smart/strict,默认smart,如果simple模式katana不爬取JS解析的路径 </span></span>
<span class="line"><span> -proxy      配置代理地址，支持扫描器、流量转发器、Burp、yakit等 </span></span>
<span class="line"><span> -blackKey   黑名单关键词，用于避免被爬虫执行危险操作，用,分割，如：logout,delete,update </span></span>
<span class="line"><span> -url        执行爬行的单个URL </span></span>
<span class="line"><span> -urlTxtPath 如果需求是批量爬行URL，那需要将URL写入txt，然后放txt路径 </span></span>
<span class="line"><span> -encodeUrlWithCharset  是否对URL进行编码，Crwalergo的功能但katana跑完的结果走Crawlergo后也会被编码 </span></span>
<span class="line"><span> -depth      爬行深度，默认3</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p><strong>不联动其他工具：</strong></p><div class="language- line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki one-dark-pro vp-code" tabindex="0"><code><span class="line"><span>.\\Venom.exe -urlTxtPath .\\text.txt.\\Vebom.exe -url    https://www.sf-express.com</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><img src="`+r+'" alt="图片"></p><p><img src="'+p+'" alt="图片"></p><p><strong>联动其他工具：</strong></p><div class="language- line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki one-dark-pro vp-code" tabindex="0"><code><span class="line"><span>.\\Venom.exe -urlTxtPath .\\text.txt -proxy http://127.0.0.1:9090.\\Vebom.exe -url  https://www.sf-express.com -proxy http://127.0.0.1:9090</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><img src="'+t+'" alt="图片"></p><p>上图的使用思路将，爬虫爬取的URL通过代理转发器给流量转发器，再由流量转发器转发器给代理工具/漏扫。</p><p>如果想在爬取过程中查看爬行效果的话，可以在命令后面带上-headless就会启动浏览器界面。</p><p><img src="'+o+'" alt="图片"></p><p>这还不开启捡洞模式？？？</p><p><strong>注意事项</strong></p><p>浏览器上下文创建错误：exec: &quot;google-chrome&quot;: executable file not found in %path%：</p><p>浏览说明设备没有安装或者%path%环境里面没有chromium的地址（用edge/chrome/chromium都可以解决）。</p><p><img src="'+l+'" alt="图片"></p><p><img src="'+i+'" alt="图片"></p><p>这里搭配-chromium参数即可。</p><p><strong>下载地址</strong>：<strong><a href="https://github.com/z-bool/Venom-Crawler" target="_blank" rel="noreferrer">https://github.com/z-bool/Venom-Crawler</a></strong></p>',29),d=[m];function u(b,g,_,h,w,x){return e(),a("div",null,d)}const V=s(c,[["render",u]]);export{v as __pageData,V as default};
